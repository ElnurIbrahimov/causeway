"""
Transformer Bridge: Wires Causeway to a frozen Transformer backbone.

The bridge:
    1. Extracts hidden states from the frozen Transformer
    2. Extracts action representations from the Transformer's candidate output
    3. Feeds both through Causeway
    4. Injects the delta vector back into the Transformer as structured prefix tokens

Injection mechanism:
    The delta vector is projected into the Transformer's embedding space as
    a set of "causal prefix" tokens. These are prepended to the input at the
    next generation step, allowing the Transformer to condition on causal
    information without any modification to its weights.

    This is similar to prefix tuning (Li & Liang, 2021) but the prefix
    content is dynamically generated by Causeway rather than learned directly.

    delta ∈ R^5 → project to R^{n_prefix × d_model} → prepend to input

Usage:
    bridge = TransformerBridge(
        causeway=causeway,
        d_model=768,
        n_prefix_tokens=4,
    )

    # During generation:
    prefix = bridge.get_causal_prefix(hidden_states, action_repr)
    # Prepend prefix to the next input embedding
"""

import torch
import torch.nn as nn
from typing import Optional

from causeway.causeway_module import Causeway
from causeway.delta_predictor import DeltaVector


class TransformerBridge(nn.Module):
    """
    Bridges Causeway to a frozen Transformer.

    Converts Causeway's delta vector into prefix tokens that can be
    injected into the Transformer's input without modifying its weights.
    """

    def __init__(
        self,
        causeway: Causeway,
        d_model: int,
        n_prefix_tokens: int = 4,
        n_delta_dims: int = 5,
    ):
        """
        Args:
            causeway: The Causeway module.
            d_model: Transformer hidden dimension.
            n_prefix_tokens: Number of prefix tokens to generate per delta.
            n_delta_dims: Number of structured delta dimensions.
        """
        super().__init__()
        self.causeway = causeway
        self.d_model = d_model
        self.n_prefix_tokens = n_prefix_tokens

        # Project delta vector + confidence → prefix token embeddings
        # Input: delta values (n_delta_dims) + confidence (n_delta_dims) = 2 * n_delta_dims
        input_dim = 2 * n_delta_dims
        self.delta_to_prefix = nn.Sequential(
            nn.Linear(input_dim, d_model),
            nn.GELU(),
            nn.Linear(d_model, n_prefix_tokens * d_model),
        )

        # Learned "causal" position embeddings for prefix tokens
        self.prefix_positions = nn.Parameter(
            torch.randn(1, n_prefix_tokens, d_model) * 0.02
        )

        # Layer norm to match Transformer's expected input distribution
        self.norm = nn.LayerNorm(d_model)

    def get_delta(
        self,
        h: torch.Tensor,
        action: torch.Tensor,
    ) -> DeltaVector:
        """
        Run Causeway and return the raw delta vector.

        Args:
            h: Transformer hidden state, shape (batch, d_model).
            action: Action representation, shape (batch, d_action).

        Returns:
            DeltaVector from Causeway.
        """
        return self.causeway(h, action)

    def get_causal_prefix(
        self,
        h: torch.Tensor,
        action: torch.Tensor,
    ) -> torch.Tensor:
        """
        Generate causal prefix tokens for the Transformer.

        Args:
            h: Transformer hidden state, shape (batch, d_model).
            action: Action representation, shape (batch, d_action).

        Returns:
            prefix: Causal prefix tokens, shape (batch, n_prefix_tokens, d_model).
                    These should be prepended to the Transformer's input embeddings.
        """
        batch_size = h.shape[0]

        # Run Causeway
        delta = self.causeway(h, action)

        # Combine delta values and confidence
        delta_input = torch.cat([delta.values, delta.confidence], dim=-1)

        # Project to prefix token space
        prefix_flat = self.delta_to_prefix(delta_input)  # (batch, n_prefix * d_model)
        prefix = prefix_flat.view(batch_size, self.n_prefix_tokens, self.d_model)

        # Add positional information and normalize
        prefix = prefix + self.prefix_positions
        prefix = self.norm(prefix)

        return prefix

    def forward(
        self,
        input_embeddings: torch.Tensor,
        h: torch.Tensor,
        action: torch.Tensor,
    ) -> torch.Tensor:
        """
        Prepend causal prefix to input embeddings.

        Args:
            input_embeddings: Original Transformer input, (batch, seq_len, d_model).
            h: Hidden state for Causeway, (batch, d_model).
            action: Action representation, (batch, d_action).

        Returns:
            augmented_embeddings: (batch, n_prefix + seq_len, d_model).
        """
        prefix = self.get_causal_prefix(h, action)
        return torch.cat([prefix, input_embeddings], dim=1)


class TransformerBridgeV2(nn.Module):
    """
    V2 Bridge: Uses full causal representations, not just the 10-dim delta summary.

    Input to prefix generator:
        cat([z_refined, z_counterfactual, delta.values, delta.confidence])
        = 2 * d_causal + 2 * n_delta_dims

    This gives the bridge access to the full learned causal state, not just
    the projected summary. The bridge can learn which causal variables matter
    for steering the Transformer.
    """

    def __init__(
        self,
        causeway: Causeway,
        d_model: int,
        d_causal: int,
        n_prefix_tokens: int = 4,
        n_delta_dims: int = 5,
    ):
        super().__init__()
        self.causeway = causeway
        self.d_model = d_model
        self.d_causal = d_causal
        self.n_prefix_tokens = n_prefix_tokens

        # Full causal state as input: z_refined + z_counterfactual + delta + confidence
        input_dim = 2 * d_causal + 2 * n_delta_dims

        self.prefix_generator = nn.Sequential(
            nn.Linear(input_dim, d_model),
            nn.GELU(),
            nn.Linear(d_model, n_prefix_tokens * d_model),
        )

        # Learned positional embeddings for prefix tokens
        self.prefix_positions = nn.Parameter(
            torch.randn(1, n_prefix_tokens, d_model) * 0.02
        )

        # LayerNorm to match Transformer's expected input distribution
        self.norm = nn.LayerNorm(d_model)

    def get_delta(self, h: torch.Tensor, action: torch.Tensor) -> DeltaVector:
        """Run Causeway and return the raw delta vector."""
        return self.causeway(h, action)

    def get_causal_prefix(
        self,
        h: torch.Tensor,
        action: torch.Tensor,
    ) -> torch.Tensor:
        """
        Generate causal prefix tokens using full causal representations.

        Args:
            h: Transformer hidden state, shape (batch, d_model).
            action: Action representation, shape (batch, d_action).

        Returns:
            prefix: shape (batch, n_prefix_tokens, d_model).
        """
        batch_size = h.shape[0]

        # Get full internals from Causeway
        internals = self.causeway.forward_with_internals(h, action)
        delta = internals['delta']
        z_refined = internals['z_refined']
        z_counterfactual = internals['z_counterfactual']

        # Build wide input: full causal state + delta summary
        bridge_input = torch.cat([
            z_refined,
            z_counterfactual,
            delta.values,
            delta.confidence,
        ], dim=-1)

        # Generate prefix tokens
        prefix_flat = self.prefix_generator(bridge_input)
        prefix = prefix_flat.view(batch_size, self.n_prefix_tokens, self.d_model)

        # Add positional info and normalize
        prefix = prefix + self.prefix_positions
        prefix = self.norm(prefix)

        return prefix

    def get_causal_prefix_from_internals(
        self,
        z_refined: torch.Tensor,
        z_counterfactual: torch.Tensor,
        delta: DeltaVector,
    ) -> torch.Tensor:
        """
        Generate prefix from pre-computed Causeway internals.
        Useful when you've already run Causeway and want to avoid re-computation.
        """
        batch_size = z_refined.shape[0]

        bridge_input = torch.cat([
            z_refined,
            z_counterfactual,
            delta.values,
            delta.confidence,
        ], dim=-1)

        prefix_flat = self.prefix_generator(bridge_input)
        prefix = prefix_flat.view(batch_size, self.n_prefix_tokens, self.d_model)
        prefix = prefix + self.prefix_positions
        prefix = self.norm(prefix)

        return prefix

    def forward(
        self,
        input_embeddings: torch.Tensor,
        h: torch.Tensor,
        action: torch.Tensor,
    ) -> torch.Tensor:
        """Prepend causal prefix to input embeddings."""
        prefix = self.get_causal_prefix(h, action)
        return torch.cat([prefix, input_embeddings], dim=1)
