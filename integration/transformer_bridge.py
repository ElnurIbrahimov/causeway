"""
Transformer Bridge: Wires Causeway to a frozen Transformer backbone.

The bridge:
    1. Extracts hidden states from the frozen Transformer
    2. Extracts action representations from the Transformer's candidate output
    3. Feeds both through Causeway
    4. Injects the delta vector back into the Transformer as structured prefix tokens

Injection mechanism:
    The delta vector is projected into the Transformer's embedding space as
    a set of "causal prefix" tokens. These are prepended to the input at the
    next generation step, allowing the Transformer to condition on causal
    information without any modification to its weights.

    This is similar to prefix tuning (Li & Liang, 2021) but the prefix
    content is dynamically generated by Causeway rather than learned directly.

    delta ∈ R^5 → project to R^{n_prefix × d_model} → prepend to input

Usage:
    bridge = TransformerBridge(
        causeway=causeway,
        d_model=768,
        n_prefix_tokens=4,
    )

    # During generation:
    prefix = bridge.get_causal_prefix(hidden_states, action_repr)
    # Prepend prefix to the next input embedding
"""

import torch
import torch.nn as nn
from typing import Optional

from causeway.causeway_module import Causeway
from causeway.delta_predictor import DeltaVector


class TransformerBridge(nn.Module):
    """
    Bridges Causeway to a frozen Transformer.

    Converts Causeway's delta vector into prefix tokens that can be
    injected into the Transformer's input without modifying its weights.
    """

    def __init__(
        self,
        causeway: Causeway,
        d_model: int,
        n_prefix_tokens: int = 4,
        n_delta_dims: int = 5,
    ):
        """
        Args:
            causeway: The Causeway module.
            d_model: Transformer hidden dimension.
            n_prefix_tokens: Number of prefix tokens to generate per delta.
            n_delta_dims: Number of structured delta dimensions.
        """
        super().__init__()
        self.causeway = causeway
        self.d_model = d_model
        self.n_prefix_tokens = n_prefix_tokens

        # Project delta vector + confidence → prefix token embeddings
        # Input: delta values (n_delta_dims) + confidence (n_delta_dims) = 2 * n_delta_dims
        input_dim = 2 * n_delta_dims
        self.delta_to_prefix = nn.Sequential(
            nn.Linear(input_dim, d_model),
            nn.GELU(),
            nn.Linear(d_model, n_prefix_tokens * d_model),
        )

        # Learned "causal" position embeddings for prefix tokens
        self.prefix_positions = nn.Parameter(
            torch.randn(1, n_prefix_tokens, d_model) * 0.02
        )

        # Layer norm to match Transformer's expected input distribution
        self.norm = nn.LayerNorm(d_model)

    def get_delta(
        self,
        h: torch.Tensor,
        action: torch.Tensor,
    ) -> DeltaVector:
        """
        Run Causeway and return the raw delta vector.

        Args:
            h: Transformer hidden state, shape (batch, d_model).
            action: Action representation, shape (batch, d_action).

        Returns:
            DeltaVector from Causeway.
        """
        return self.causeway(h, action)

    def get_causal_prefix(
        self,
        h: torch.Tensor,
        action: torch.Tensor,
    ) -> torch.Tensor:
        """
        Generate causal prefix tokens for the Transformer.

        Args:
            h: Transformer hidden state, shape (batch, d_model).
            action: Action representation, shape (batch, d_action).

        Returns:
            prefix: Causal prefix tokens, shape (batch, n_prefix_tokens, d_model).
                    These should be prepended to the Transformer's input embeddings.
        """
        batch_size = h.shape[0]

        # Run Causeway
        delta = self.causeway(h, action)

        # Combine delta values and confidence
        delta_input = torch.cat([delta.values, delta.confidence], dim=-1)

        # Project to prefix token space
        prefix_flat = self.delta_to_prefix(delta_input)  # (batch, n_prefix * d_model)
        prefix = prefix_flat.view(batch_size, self.n_prefix_tokens, self.d_model)

        # Add positional information and normalize
        prefix = prefix + self.prefix_positions
        prefix = self.norm(prefix)

        return prefix

    def forward(
        self,
        input_embeddings: torch.Tensor,
        h: torch.Tensor,
        action: torch.Tensor,
    ) -> torch.Tensor:
        """
        Prepend causal prefix to input embeddings.

        Args:
            input_embeddings: Original Transformer input, (batch, seq_len, d_model).
            h: Hidden state for Causeway, (batch, d_model).
            action: Action representation, (batch, d_action).

        Returns:
            augmented_embeddings: (batch, n_prefix + seq_len, d_model).
        """
        prefix = self.get_causal_prefix(h, action)
        return torch.cat([prefix, input_embeddings], dim=1)
